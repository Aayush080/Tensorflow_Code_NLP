{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Logs dictionary.ipynb","provenance":[{"file_id":"1jepIhnWcguzdltyGkufBsf_Toz6wbhK_","timestamp":1618118528037}],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"GpySdgYuYFwR"},"source":["# Using the logs dictionary\n","\n","In this reading, we will learn how to take advantage of the `logs` dictionary in Keras to define our own callbacks and check the progress of a model."]},{"cell_type":"code","metadata":{"id":"gdVLVTIOYFwW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618118557621,"user_tz":-330,"elapsed":2387,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"ac89d01d-7021-4e74-900c-a3e10187a197"},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.4.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8f5PmzkkYFwW"},"source":["The `logs` dictionary stores the loss value, along with all of the metrics we are using at the end of a batch or epoch.\n","\n","We can incorporate information from the `logs` dictionary into our own custom callbacks.\n","\n","Let's see this in action in the context of a model we will construct and fit to the `sklearn` diabetes dataset that we have been using in this module.\n","\n","Let's first import the dataset, and split it into the training and test sets."]},{"cell_type":"code","metadata":{"id":"JXQv6mHEYFwX","executionInfo":{"status":"ok","timestamp":1618118609217,"user_tz":-330,"elapsed":1688,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Load the diabetes dataset\n","\n","from sklearn.datasets import load_diabetes\n","\n","diabetes_dataset = load_diabetes()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a5tC-h8YFwX","executionInfo":{"status":"ok","timestamp":1618118609217,"user_tz":-330,"elapsed":874,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Save the input and target variables\n","\n","from sklearn.model_selection import train_test_split\n","\n","data = diabetes_dataset['data']\n","targets = diabetes_dataset['target']"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"fL6Gr7keYFwX","executionInfo":{"status":"ok","timestamp":1618118610290,"user_tz":-330,"elapsed":1161,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Split the data set into training and test sets\n","\n","train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flT4NG6RYFwY"},"source":["Now we construct our model."]},{"cell_type":"code","metadata":{"id":"Z7k5uQOyYFwY","executionInfo":{"status":"ok","timestamp":1618118613817,"user_tz":-330,"elapsed":1299,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Build the model\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = tf.keras.Sequential([\n","    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n","    Dense(64,activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    Dense(64, activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(1)        \n","])"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KoOrDsByYFwY"},"source":["We now compile the model, with\n","* Mean squared error as the loss function,\n","* the Adam optimizer, and \n","* Mean absolute error (`mae`) as a metric."]},{"cell_type":"code","metadata":{"id":"EUcyZ7B2YFwY","executionInfo":{"status":"ok","timestamp":1618118620903,"user_tz":-330,"elapsed":1019,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Compile the model\n","    \n","model.compile(loss='mse', optimizer=\"adam\", metrics=['mae'])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RjCWARysYFwZ"},"source":["### Defining a custom callback\n","\n","Now we define our custom callback using the `logs` dictionary to access the loss and metric values."]},{"cell_type":"code","metadata":{"id":"nisLthlEYFwZ","executionInfo":{"status":"ok","timestamp":1618118950759,"user_tz":-330,"elapsed":1078,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Create the custom callback\n","\n","class LossAndMetricCallback(tf.keras.callbacks.Callback):\n","\n","    # Print the loss after every second batch in the training set\n","    def on_train_batch_end(self, batch, logs=None):\n","        if batch %2 ==0:\n","            print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n","    \n","    # Print the loss after each batch in the test set\n","    def on_test_batch_end(self, batch, logs=None):\n","        print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss']))\n","\n","    # Print the loss and mean absolute error after each epoch\n","    def on_epoch_end(self, epoch, logs=None):\n","        print('Epoch {}: Average loss is {:7.2f}, mean absolute error is {:7.2f}.'.format(epoch, logs['loss'], logs['mae']))\n","    \n","    # Notify the user when prediction has finished on each batch\n","    def on_predict_batch_end(self,batch, logs=None):\n","        print(\"Finished prediction on batch {}!\".format(batch))"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A1GnCKPzYFwZ"},"source":["We now fit the model to the data, and specify that we would like to use our custom callback `LossAndMetricCallback()`."]},{"cell_type":"code","metadata":{"id":"OaO06aDMYFwZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618118955582,"user_tz":-330,"elapsed":2153,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"673b54e9-1179-4ade-bdff-209a4f51690f"},"source":["# Train the model\n","\n","history = model.fit(train_data, train_targets, epochs=20,\n","                    batch_size=100, callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n"," After batch 0, the loss is 26616.67.\n","\n"," After batch 2, the loss is 28578.21.\n","Epoch 0: Average loss is 29415.42, mean absolute error is  153.10.\n","\n"," After batch 0, the loss is 26431.02.\n","\n"," After batch 2, the loss is 30164.05.\n","Epoch 1: Average loss is 29317.76, mean absolute error is  152.79.\n","\n"," After batch 0, the loss is 32290.90.\n","\n"," After batch 2, the loss is 29537.91.\n","Epoch 2: Average loss is 29188.18, mean absolute error is  152.38.\n","\n"," After batch 0, the loss is 32827.78.\n","\n"," After batch 2, the loss is 30321.53.\n","Epoch 3: Average loss is 28997.38, mean absolute error is  151.76.\n","\n"," After batch 0, the loss is 29887.60.\n","\n"," After batch 2, the loss is 29356.59.\n","Epoch 4: Average loss is 28716.69, mean absolute error is  150.85.\n","\n"," After batch 0, the loss is 26010.31.\n","\n"," After batch 2, the loss is 28315.76.\n","Epoch 5: Average loss is 28312.86, mean absolute error is  149.56.\n","\n"," After batch 0, the loss is 26090.11.\n","\n"," After batch 2, the loss is 27318.62.\n","Epoch 6: Average loss is 27748.25, mean absolute error is  147.76.\n","\n"," After batch 0, the loss is 32186.31.\n","\n"," After batch 2, the loss is 27784.50.\n","Epoch 7: Average loss is 27022.75, mean absolute error is  145.40.\n","\n"," After batch 0, the loss is 27947.48.\n","\n"," After batch 2, the loss is 26614.01.\n","Epoch 8: Average loss is 26066.95, mean absolute error is  142.30.\n","\n"," After batch 0, the loss is 24160.26.\n","\n"," After batch 2, the loss is 24983.92.\n","Epoch 9: Average loss is 24876.58, mean absolute error is  138.30.\n","\n"," After batch 0, the loss is 24265.39.\n","\n"," After batch 2, the loss is 23988.96.\n","Epoch 10: Average loss is 23439.70, mean absolute error is  133.25.\n","\n"," After batch 0, the loss is 21604.27.\n","\n"," After batch 2, the loss is 23003.75.\n","Epoch 11: Average loss is 21699.96, mean absolute error is  126.88.\n","\n"," After batch 0, the loss is 18451.12.\n","\n"," After batch 2, the loss is 19674.44.\n","Epoch 12: Average loss is 19650.26, mean absolute error is  119.24.\n","\n"," After batch 0, the loss is 20747.50.\n","\n"," After batch 2, the loss is 18384.73.\n","Epoch 13: Average loss is 17435.14, mean absolute error is  109.97.\n","\n"," After batch 0, the loss is 19143.70.\n","\n"," After batch 2, the loss is 15809.79.\n","Epoch 14: Average loss is 15363.33, mean absolute error is  101.21.\n","\n"," After batch 0, the loss is 12928.15.\n","\n"," After batch 2, the loss is 13100.47.\n","Epoch 15: Average loss is 12784.45, mean absolute error is   90.87.\n","\n"," After batch 0, the loss is 13808.68.\n","\n"," After batch 2, the loss is 11417.06.\n","Epoch 16: Average loss is 10682.91, mean absolute error is   81.98.\n","\n"," After batch 0, the loss is 11342.07.\n","\n"," After batch 2, the loss is 8869.32.\n","Epoch 17: Average loss is 8981.99, mean absolute error is   74.32.\n","\n"," After batch 0, the loss is 9997.92.\n","\n"," After batch 2, the loss is 8624.65.\n","Epoch 18: Average loss is 7739.85, mean absolute error is   68.67.\n","\n"," After batch 0, the loss is 7883.28.\n","\n"," After batch 2, the loss is 6516.11.\n","Epoch 19: Average loss is 7034.78, mean absolute error is   65.81.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"V3i9w9MNYFwZ"},"source":["We can also use our callback in the `evaluate` function..."]},{"cell_type":"code","metadata":{"id":"zlhxkGG8YFwa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618118991499,"user_tz":-330,"elapsed":1029,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"99524e7f-6355-4fa4-d193-50b6110a0a3c"},"source":["# Evaluate the model\n","\n","model_eval = model.evaluate(test_data, test_targets, batch_size=10, \n","                            callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["\n"," After batch 0, the loss is 19699.67.\n","\n"," After batch 1, the loss is 16595.28.\n","\n"," After batch 2, the loss is 18961.87.\n","\n"," After batch 3, the loss is 16410.48.\n","\n"," After batch 4, the loss is 16563.52.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oVIgg2z4YFwa"},"source":["...And also the `predict` function."]},{"cell_type":"code","metadata":{"id":"grzQVXuRYFwa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618119000255,"user_tz":-330,"elapsed":1402,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"6df15e7d-5221-4f40-f7b3-ea00a9625401"},"source":["# Get predictions from the model\n","\n","model_pred = model.predict(test_data, batch_size=10,\n","                           callbacks=[LossAndMetricCallback()], verbose=False)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Finished prediction on batch 0!\n","Finished prediction on batch 1!\n","Finished prediction on batch 2!\n","Finished prediction on batch 3!\n","Finished prediction on batch 4!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tOfJ03lBYFwa"},"source":["### Application - learning rate scheduler\n","Let's now look at a more sophisticated custom callback. \n","\n","We are going to define a callback to change the learning rate of the optimiser of a model during training. We will do this by specifying the epochs and new learning rates where we would like it to be changed.\n","\n","First we define the auxillary function that returns the learning rate for each epoch based on our schedule."]},{"cell_type":"code","metadata":{"id":"F9D9UOlbYFwa","executionInfo":{"status":"ok","timestamp":1618119352762,"user_tz":-330,"elapsed":1096,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Define the learning rate schedule. The tuples below are (start_epoch, new_learning_rate)\n","\n","lr_schedule = [\n","    (4, 0.03), (7, 0.02), (11, 0.005), (15, 0.007)\n","]\n","\n","def get_new_epoch_lr(epoch, lr):\n","    # Checks to see if the input epoch is listed in the learning rate schedule \n","    # and if so, returns index in lr_schedule\n","    epoch_in_sched = [i for i in range(len(lr_schedule)) if lr_schedule[i][0]==int(epoch)]\n","    if len(epoch_in_sched)>0:\n","        # If it is, return the learning rate corresponding to the epoch\n","        return lr_schedule[epoch_in_sched[0]][1]\n","    else:\n","        # Otherwise, return the existing learning rate\n","        return lr"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Dhr0vWYzYFwb"},"source":["Let's now define the callback itself."]},{"cell_type":"code","metadata":{"id":"GZyJr40xYFwb","executionInfo":{"status":"ok","timestamp":1618119356759,"user_tz":-330,"elapsed":1383,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Define the custom callback\n","\n","class LRScheduler(tf.keras.callbacks.Callback):\n","    \n","    def __init__(self, new_lr):\n","        super(LRScheduler, self).__init__()\n","        # Add the new learning rate function to our callback\n","        self.new_lr = new_lr\n","\n","    def on_epoch_begin(self, epoch, logs=None):\n","        # Make sure that the optimizer we have chosen has a learning rate, and raise an error if not\n","        if not hasattr(self.model.optimizer, 'lr'):\n","              raise ValueError('Error: Optimizer does not have a learning rate.')\n","                \n","        # Get the current learning rate\n","        curr_rate = float(tf.keras.backend.get_value(self.model.optimizer.lr))\n","        \n","        # Call the auxillary function to get the scheduled learning rate for the current epoch\n","        scheduled_rate = self.new_lr(epoch, curr_rate)\n","\n","        # Set the learning rate to the scheduled learning rate\n","        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_rate)\n","        print('Learning rate for epoch {} is {:7.3f}'.format(epoch, scheduled_rate))"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jpCy6J2pYFwb"},"source":["Let's now train the model again with our new callback. "]},{"cell_type":"code","metadata":{"id":"3rkTuVMaYFwb","executionInfo":{"status":"ok","timestamp":1618119705253,"user_tz":-330,"elapsed":1197,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Build the same model as before\n","\n","new_model = tf.keras.Sequential([\n","    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n","    Dense(64,activation='relu'),\n","    tf.keras.layers.BatchNormalization(),\n","    Dense(64, activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(1)        \n","])"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7X-1YuCYFwc","executionInfo":{"status":"ok","timestamp":1618119707720,"user_tz":-330,"elapsed":1228,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Compile the model\n","\n","new_model.compile(loss='mse',\n","                optimizer=\"adam\",\n","                metrics=['mae', 'mse'])"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"wqUS4ViRYFwc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1618119770393,"user_tz":-330,"elapsed":1362,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"cb359506-aab9-43c6-c757-29f3cd582f89"},"source":["# Fit the model with our learning rate scheduler callback\n","\n","new_history = new_model.fit(train_data, train_targets, epochs=20,\n","                            batch_size=100, callbacks=[LRScheduler(get_new_epoch_lr)], verbose=False)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Learning rate for epoch 0 is   0.007\n","Learning rate for epoch 1 is   0.007\n","Learning rate for epoch 2 is   0.007\n","Learning rate for epoch 3 is   0.007\n","Learning rate for epoch 4 is   0.030\n","Learning rate for epoch 5 is   0.030\n","Learning rate for epoch 6 is   0.030\n","Learning rate for epoch 7 is   0.020\n","Learning rate for epoch 8 is   0.020\n","Learning rate for epoch 9 is   0.020\n","Learning rate for epoch 10 is   0.020\n","Learning rate for epoch 11 is   0.005\n","Learning rate for epoch 12 is   0.005\n","Learning rate for epoch 13 is   0.005\n","Learning rate for epoch 14 is   0.005\n","Learning rate for epoch 15 is   0.007\n","Learning rate for epoch 16 is   0.007\n","Learning rate for epoch 17 is   0.007\n","Learning rate for epoch 18 is   0.007\n","Learning rate for epoch 19 is   0.007\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GtwSCruRYFwc"},"source":["### Further reading and resources\n","* https://www.tensorflow.org/guide/keras/custom_callback\n","* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/Callback"]},{"cell_type":"code","metadata":{"id":"_RBZ5pdWdGV4"},"source":[""],"execution_count":null,"outputs":[]}]}