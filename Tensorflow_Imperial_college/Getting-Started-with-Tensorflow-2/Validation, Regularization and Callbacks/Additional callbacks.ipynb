{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"Copy of Additional callbacks.ipynb","provenance":[{"file_id":"1JC780dIqlK4Moip1jJjRWn6ZiXdNYEST","timestamp":1618122421976}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"397AirEIk3TV"},"source":["# Additional callbacks\n","\n","In this reading we'll be looking at more of the inbuilt callbacks available in Keras."]},{"cell_type":"code","metadata":{"id":"_gc8hLBXk3Te","executionInfo":{"status":"ok","timestamp":1618121891878,"user_tz":-330,"elapsed":5094,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"d9ac4324-e96f-4ded-e007-bda4158368b8","colab":{"base_uri":"https://localhost:8080/"}},"source":["import tensorflow as tf\n","print(tf.__version__)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["2.4.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DQ6f6jmZk3Tf"},"source":["We will again be using the sklearn diabetes dataset to demonstrate these callbacks."]},{"cell_type":"code","metadata":{"id":"tqed4O1tk3Tf","executionInfo":{"status":"ok","timestamp":1618121897161,"user_tz":-330,"elapsed":2873,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Load the diabetes dataset\n","\n","from sklearn.datasets import load_diabetes\n","\n","diabetes_dataset = load_diabetes()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"cCx-mXHFk3Tg","executionInfo":{"status":"ok","timestamp":1618121898224,"user_tz":-330,"elapsed":3390,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Save the input and target variables\n","\n","from sklearn.model_selection import train_test_split\n","\n","data = diabetes_dataset['data']\n","targets = diabetes_dataset['target']"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"euEalqcek3Tg","executionInfo":{"status":"ok","timestamp":1618121898225,"user_tz":-330,"elapsed":2805,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Split the data set into training and test sets\n","\n","train_data, test_data, train_targets, test_targets = train_test_split(data, targets, test_size=0.1)"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DimbAkh7k3Th"},"source":["Let's also build a simple model to fit to the data with our callbacks."]},{"cell_type":"code","metadata":{"id":"jApAi-_uk3Th","executionInfo":{"status":"ok","timestamp":1618121899820,"user_tz":-330,"elapsed":3199,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Build the model\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","\n","model = tf.keras.Sequential([\n","    Dense(128, activation='relu', input_shape=(train_data.shape[1],)),\n","    Dense(64,activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(1)        \n","])"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vt15fr9ok3Ti","executionInfo":{"status":"ok","timestamp":1618121900465,"user_tz":-330,"elapsed":2323,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Compile the model\n","\n","model.compile(loss='mse',\n","                optimizer=\"adam\",metrics=[\"mse\",\"mae\"])"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"He5hnRUZk3Ti"},"source":["Now onto the callbacks!"]},{"cell_type":"markdown","metadata":{"id":"jM5C64qDk3Ti"},"source":["## Learning rate scheduler\n","\n","**Usage:** `tf.keras.callbacks.LearningRateScheduler(schedule, verbose=0)`\n","\n","The learning rate scheduler that we implemented in the previous reading as a custom callback is also available as a built in callback. \n","\n","As in our custom callback, the `LearningRateScheduler` in Keras takes a function `schedule` as an argument. \n","\n","This function `schedule` should take two arguments:\n","* The current epoch (as an integer), and\n","* The current learning rate,\n","\n","and return new learning rate for that epoch. \n","\n","The `LearningRateScheduler` also has an optional `verbose` argument, which prints information about the learning rate if it is set to 1.\n","\n","Let's see a simple example."]},{"cell_type":"code","metadata":{"id":"uOK0WGM9k3Tj","executionInfo":{"status":"ok","timestamp":1618121965601,"user_tz":-330,"elapsed":1389,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Define the learning rate schedule function\n","\n","def lr_function(epoch, lr):\n","    if epoch % 2 == 0:\n","        return lr\n","    else:\n","        return lr + epoch/1000"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMPxMucYk3Tj","executionInfo":{"status":"ok","timestamp":1618122032208,"user_tz":-330,"elapsed":1710,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"fa020550-caf2-4929-c34a-94cbd86d66d6","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Train the model\n","\n","history = model.fit(train_data, train_targets, epochs=10,\n","                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lr_function, verbose=1)], verbose=False)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\n","Epoch 00001: LearningRateScheduler reducing learning rate to 0.0010000000474974513.\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 0.0020000000474974513.\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 0.0020000000949949026.\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 0.005000000094994903.\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 0.004999999888241291.\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 0.009999999888241292.\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 0.009999999776482582.\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 0.01699999977648258.\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 0.016999999061226845.\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 0.025999999061226846.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"d8k7INLCk3Tk"},"source":["You can also use lambda functions to define your `schedule` given an epoch."]},{"cell_type":"code","metadata":{"id":"mm923B3Uk3Tk","executionInfo":{"status":"ok","timestamp":1618122119590,"user_tz":-330,"elapsed":1243,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"a7d67d0e-426f-4000-fa9c-cc3a3d1b11b5","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Train the model with a difference schedule\n","\n","history = model.fit(train_data, train_targets, epochs=10,\n","                    callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda x:1/(3+5*x), verbose=1)], \n","                    verbose=False)"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\n","Epoch 00001: LearningRateScheduler reducing learning rate to 0.3333333333333333.\n","\n","Epoch 00002: LearningRateScheduler reducing learning rate to 0.125.\n","\n","Epoch 00003: LearningRateScheduler reducing learning rate to 0.07692307692307693.\n","\n","Epoch 00004: LearningRateScheduler reducing learning rate to 0.05555555555555555.\n","\n","Epoch 00005: LearningRateScheduler reducing learning rate to 0.043478260869565216.\n","\n","Epoch 00006: LearningRateScheduler reducing learning rate to 0.03571428571428571.\n","\n","Epoch 00007: LearningRateScheduler reducing learning rate to 0.030303030303030304.\n","\n","Epoch 00008: LearningRateScheduler reducing learning rate to 0.02631578947368421.\n","\n","Epoch 00009: LearningRateScheduler reducing learning rate to 0.023255813953488372.\n","\n","Epoch 00010: LearningRateScheduler reducing learning rate to 0.020833333333333332.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xJo--zO-k3Tk"},"source":["## CSV logger\n","**Usage** `tf.keras.callbacks.CSVLogger(filename, separator=',', append=False)`\n","\n","This callback streams the results from each epoch into a CSV file.\n","The first line of the CSV file will be the names of pieces of information recorded on each subsequent line, beginning with the epoch and loss value. The values of metrics at the end of each epoch will also be recorded.\n","\n","The only compulsory argument is the `filename` for the log to be streamed to. This could also be a filepath.\n","\n","You can also specify the `separator` to be used between entries on each line.\n","\n","The `append` argument allows you the option to append your results to an existing file with the same name. This can be particularly useful if you are continuing training.\n","\n","Let's see an example."]},{"cell_type":"code","metadata":{"id":"BQjnIifak3Tl","executionInfo":{"status":"ok","timestamp":1618122322966,"user_tz":-330,"elapsed":1192,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Train the model with a CSV logger\n","\n","history = model.fit(train_data, train_targets, epochs=10,\n","                    callbacks=[tf.keras.callbacks.CSVLogger(\"results.csv\")], verbose=False)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7bXR3fWPk3Tl"},"source":["Let's view the information in the CSV file we have created using `pandas`."]},{"cell_type":"code","metadata":{"id":"wNjh0zjuk3Tl","executionInfo":{"status":"ok","timestamp":1618122332942,"user_tz":-330,"elapsed":1083,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"4b5f850c-ae65-4e9c-b680-f28b8ebd6325","colab":{"base_uri":"https://localhost:8080/","height":373}},"source":["# Load the CSV\n","\n","import pandas as pd\n","\n","pd.read_csv(\"results.csv\", index_col='epoch')"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>loss</th>\n","      <th>mae</th>\n","      <th>mse</th>\n","    </tr>\n","    <tr>\n","      <th>epoch</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>6071.255859</td>\n","      <td>67.076782</td>\n","      <td>6071.255859</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>6069.355957</td>\n","      <td>67.060928</td>\n","      <td>6069.355957</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>6071.485352</td>\n","      <td>67.105431</td>\n","      <td>6071.485352</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>6069.955078</td>\n","      <td>67.106361</td>\n","      <td>6069.955078</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>6070.428223</td>\n","      <td>67.106194</td>\n","      <td>6070.428223</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>6070.476074</td>\n","      <td>67.111938</td>\n","      <td>6070.476074</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6068.783203</td>\n","      <td>67.050652</td>\n","      <td>6068.783203</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>6067.311035</td>\n","      <td>66.969574</td>\n","      <td>6067.311035</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>6068.622070</td>\n","      <td>66.904488</td>\n","      <td>6068.622070</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>6072.736328</td>\n","      <td>66.872856</td>\n","      <td>6072.736328</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              loss        mae          mse\n","epoch                                     \n","0      6071.255859  67.076782  6071.255859\n","1      6069.355957  67.060928  6069.355957\n","2      6071.485352  67.105431  6071.485352\n","3      6069.955078  67.106361  6069.955078\n","4      6070.428223  67.106194  6070.428223\n","5      6070.476074  67.111938  6070.476074\n","6      6068.783203  67.050652  6068.783203\n","7      6067.311035  66.969574  6067.311035\n","8      6068.622070  66.904488  6068.622070\n","9      6072.736328  66.872856  6072.736328"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"HOV3r-XNk3Tm"},"source":["## Lambda callbacks\n","**Usage** `tf.keras.callbacks.LambdaCallback(\n","        on_epoch_begin=None, on_epoch_end=None, \n","        on_batch_begin=None, on_batch_end=None, \n","        on_train_begin=None, on_train_end=None)`\n","\n","Lambda callbacks are used to quickly define simple custom callbacks with the use of lambda functions.\n","\n","Each of the functions require some positional arguments.\n","* `on_epoch_begin` and `on_epoch_end` expect two arguments: `epoch` and `logs`,\n","* `on_batch_begin` and `on_batch_end` expect two arguments: `batch` and `logs` and\n","* `on_train_begin` and `on_train_end` expect one argument: `logs`.\n","\n","Let's see an example of this in practice."]},{"cell_type":"code","metadata":{"id":"pTqp3j-Ok3Tm","executionInfo":{"status":"ok","timestamp":1618122378026,"user_tz":-330,"elapsed":1416,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Print the epoch number at the beginning of each epoch\n","\n","epoch_callback = tf.keras.callbacks.LambdaCallback(\n","    on_epoch_begin=lambda epoch,logs: print('Starting Epoch {}!'.format(epoch+1)))"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_sVU7p-k3Tn","executionInfo":{"status":"ok","timestamp":1618122378866,"user_tz":-330,"elapsed":1804,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Print the loss at the end of each batch\n","\n","batch_loss_callback = tf.keras.callbacks.LambdaCallback(\n","    on_batch_end=lambda batch,logs: print('\\n After batch {}, the loss is {:7.2f}.'.format(batch, logs['loss'])))"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y_pN7t4lk3Tn","executionInfo":{"status":"ok","timestamp":1618122379731,"user_tz":-330,"elapsed":2201,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}}},"source":["# Inform that training is finished\n","\n","train_finish_callback = tf.keras.callbacks.LambdaCallback(\n","    on_train_end=lambda logs: print('Training finished!'))"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"fHDq10xFk3Tn","executionInfo":{"status":"ok","timestamp":1618122379733,"user_tz":-330,"elapsed":1746,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"1ded4ad3-9961-4e5c-edbe-1c58477ee957","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Train the model with the lambda callbacks\n","\n","history = model.fit(train_data, train_targets, epochs=5, batch_size=100,\n","                    callbacks=[epoch_callback, batch_loss_callback,train_finish_callback], verbose=False)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Starting Epoch 1!\n","\n"," After batch 0, the loss is 6286.73.\n","\n"," After batch 1, the loss is 6032.70.\n","\n"," After batch 2, the loss is 5813.57.\n","\n"," After batch 3, the loss is 6068.91.\n","Starting Epoch 2!\n","\n"," After batch 0, the loss is 6556.32.\n","\n"," After batch 1, the loss is 6361.46.\n","\n"," After batch 2, the loss is 6163.74.\n","\n"," After batch 3, the loss is 6069.55.\n","Starting Epoch 3!\n","\n"," After batch 0, the loss is 6223.78.\n","\n"," After batch 1, the loss is 6159.64.\n","\n"," After batch 2, the loss is 6174.27.\n","\n"," After batch 3, the loss is 6068.29.\n","Starting Epoch 4!\n","\n"," After batch 0, the loss is 5504.18.\n","\n"," After batch 1, the loss is 5702.46.\n","\n"," After batch 2, the loss is 5823.88.\n","\n"," After batch 3, the loss is 6068.20.\n","Starting Epoch 5!\n","\n"," After batch 0, the loss is 6443.66.\n","\n"," After batch 1, the loss is 6118.99.\n","\n"," After batch 2, the loss is 6212.19.\n","\n"," After batch 3, the loss is 6068.25.\n","Training finished!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Tc03DfVrk3To"},"source":["## Reduce learning rate on plateau\n","**Usage** `tf.keras.callbacks.ReduceLROnPlateau(\n","            monitor='val_loss', \n","            factor=0.1, \n","            patience=10, \n","            verbose=0, \n","            mode='auto', \n","            min_delta=0.0001, \n","            cooldown=0, \n","            min_lr=0)`\n","\n","The `ReduceLROnPlateau` callback allows reduction of the learning rate when a metric has stopped improving. \n","The arguments are similar to those used in the `EarlyStopping` callback.\n","* The argument `monitor` is used to specify which metric to base the callback on.\n","* The `factor` is the factor by which the learning rate decreases i.e., new_lr=factor*old_lr.\n","* The `patience` is the number of epochs where there is no improvement on the monitored metric before the learning rate is reduced.\n","* The `verbose` argument will produce progress messages when set to 1.\n","* The `mode` determines whether the learning rate will decrease when the monitored quantity stops increasing (`max`) or decreasing (`min`). The `auto` setting causes the callback to infer the mode from the monitored quantity.\n","* The `min_delta` is the smallest change in the monitored quantity to be deemed an improvement.\n","* The `cooldown` is the number of epochs to wait after the learning rate is changed before the callback resumes normal operation.\n","* The `min_lr` is a lower bound on the learning rate that the callback will produce.\n","\n","Let's examine a final example."]},{"cell_type":"code","metadata":{"id":"dIUxqojCk3Tp","executionInfo":{"status":"ok","timestamp":1618122412955,"user_tz":-330,"elapsed":1985,"user":{"displayName":"aayush jain","photoUrl":"","userId":"06466801638325838312"}},"outputId":"0f032c7b-48ed-4211-858d-7fc2114c7eb0","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Train the model with the ReduceLROnPlateau callback\n","\n","history = model.fit(train_data, train_targets, epochs=100, batch_size=100,\n","                    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(\n","                        monitor=\"loss\",factor=0.2, verbose=1)], verbose=False)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["\n","Epoch 00022: ReduceLROnPlateau reducing learning rate to 0.00416666679084301.\n","\n","Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.0008333333767950535.\n","\n","Epoch 00042: ReduceLROnPlateau reducing learning rate to 0.00016666667070239783.\n","\n","Epoch 00057: ReduceLROnPlateau reducing learning rate to 3.333333297632635e-05.\n","\n","Epoch 00069: ReduceLROnPlateau reducing learning rate to 6.666666740784422e-06.\n","\n","Epoch 00079: ReduceLROnPlateau reducing learning rate to 1.3333333299669903e-06.\n","\n","Epoch 00089: ReduceLROnPlateau reducing learning rate to 2.666666659933981e-07.\n","\n","Epoch 00099: ReduceLROnPlateau reducing learning rate to 5.3333332061811235e-08.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jE7kSsi9k3Tp"},"source":["### Further reading and resources\n","* https://keras.io/callbacks/\n","* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LearningRateScheduler\n","* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/CSVLogger\n","* https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/LambdaCallback"]}]}